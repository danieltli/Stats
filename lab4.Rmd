---
title: "Lab4"
output: 
  html_document:
    toc: yes
    toc_float: yes
date: "2024-02-07"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Task 1
```{r}
getwd()
```

# Task 2
```{r}
spruce.df <- read.csv("SPRUCE.csv")
tail(spruce.df)
```

#Task 3

```{r}
library(s20x)
trendscatter(Height~BHDiameter, data =spruce.df, f = 0.5)
spruce.lm <- with(spruce.df, lm(Height~BHDiameter))
height.res <- residuals(spruce.lm)
height.fit <- fitted(spruce.lm)
plot(y = height.res, x = height.fit)
trendscatter(y = height.res, x = height.fit)
```

In the original plot, the curve still stays increasing, but the slope gets smaller. In the next plot, however, we see a decrease in the curve's value, which is because we are using 2 different sets of data(Height vs BHDiameter and Residuals vs Fitted).

```{r}
plot(spruce.lm, which = 1)
normcheck(spruce.lm, shapiro.wilk = TRUE)
round(mean(height.res),4)
```
Since there is a P-value greater than .05, we are able to accept the NULL hypothesis and consider error distributed normally.
Since the mean of the residual is 0, we know that it works better than the straight line model for prediction of the spruce height. We want to eliminate as much noise from the data as possible, and the straight line model leaves that in.

#Task 4

```{r}
quad.lm <- lm(Height~BHDiameter + I(BHDiameter^2), data = spruce.df)
summary(quad.lm)
```
```{r}
plot(spruce.df)
myplot = function(x){
  quad.lm$coefficients[1] + quad.lm$coefficients[2] *x + quad.lm$coefficients[3] * x ^2
}

curve(myplot,lwd = 2 , col= "steelblue", add = TRUE)

quad.fit = fitted(quad.lm)
plot(quad.lm, which = 1)
normcheck(quad.lm, shapiro.wilk = TRUE)

```
Since the p-value is .684, higher than the previous one, we accept the NULL hypothesis. 

We can see that the quadratic model better describes the data than the linear model.

# Task 5

```{r}
summary(quad.lm)
```

𝛽0 = .860896
𝛽1 = 1.469592
𝛽2 = -.027457

```{r}
ciReg(quad.lm)
```

Fitted line equation : .860896 + 1.46959x - .027457x^2

```{r}
predict(quad.lm, data.frame(BHDiameter = c(15,18,20)))
predict(spruce.lm, data.frame(BHDiameter = c(15,18,20)))
```

We can see that the predictions made by quad.lm are larger than the spruce.lm ones. 

```{r}
summary(spruce.lm)$r.squared
summary(quad.lm)$adj.r.squared
summary(spruce.lm)$adj.r.squared
summary(quad.lm)$r.squared
```
R^2 = .6569146


Adjusted R-squared refers to how well the model describes the data. We can see that the quadratic data adjusted for more variables performs better than the other ones by a large margin.

Multiple R-squared describes how well data fits a particular model. It does not depend on the amount or effectiveness of included variables. 
 
As we can see, quad.lm describes the variability in height better. The R-values of its adjusted and non-adjusted are both higher than spruce.lm

```{r}
anova(spruce.lm)
```
```{r}
anova(quad.lm)
```
```{r}
anova(spruce.lm,quad.lm)
```

Quad.lm fits the data better. It having the smallest RSS value makes it closer to the data than the others

```{r}
height.qfit <- fitted(quad.lm)

TSS <- with(spruce.df, sum((Height - mean(Height))^2))
MSS <- with(spruce.df, sum((height.qfit - mean(Height))^2))
RSS <- with(spruce.df, sum((Height - height.qfit)^2))
            
TSS
MSS
RSS

round(MSS/TSS,5)
```

The value of MSS/TSS is 0.77413.

# Task 6

```{r}
cooks20x(quad.lm)
```
Cook's distance is a measure of how much the data would change if a point of the data was deleted. The farther away from the rest, the greater the value.

It is used to determine outliers, or to prediect a new model without any heavy influences. 


Looking at the graph, the 24th datum is the most influential, since it has the largest height distance value.

```{r}
quad2.lm <- lm(Height~BHDiameter + I(BHDiameter^2), data = spruce.df[-24,])
summary(quad2.lm)
```
```{r}
summary(quad.lm)
```
All of the residual data point(Min, Median, etc.) of quad2 are smaller than quad's residuals. Both R^2 values are also larger.

Cook's Distance plot predicted that by removing the 24th datum, the R^2 value would increase. It was correct, and both values have become larger.


# Task 7

Suppose for lines 1 and 2 we have the formulas:

$l1: y = \beta_0 + \beta_1x$
$l1: y = \beta_0 + \delta + (\beta_1+\zeta)x$

At the change point the two lines intersect

$\beta_0 + \beta_1x_k = \beta_0 + \delta + (\beta_1 + \zeta)x_k$

After canceling, we can get

$\delta = -\zeta x_k$

We can then rewrite $l2$ as 

$y = \beta_0 - \zeta x_k + (\beta_1 + \zeta)x$

That is 

$y = \beta_0 + \beta_1 x  + \zeta(x - x_k)I(x>x_k)$

$l2$ is $l1$ with an adjustment term

We then introduce an indicator function that will be 1 when $x > x_k$ and 0 else
So we have

$y = \beta_0 + \beta_1 x + \zeta(x = x_k)I(x>x_k) $